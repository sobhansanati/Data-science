{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5d1caa31-1020-4cf4-b903-6c2d516a10e2",
   "metadata": {},
   "source": [
    "(importing the libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ba6b4d42-04cf-4629-a298-dd44a8fcf139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0105f7a3-3cb2-4722-989b-b836daeec6ee",
   "metadata": {},
   "source": [
    "loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aaf67272-72f4-47c0-9a1e-da449fc1d969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>71.0</td>\n",
       "      <td>9353</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>Oth</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9750</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>10140</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>TenC</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>257</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>TenC</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>258</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0      1          20       RL         71.0     9353   Pave   NaN      Reg   \n",
       "1      2          60       RL         80.0    10400   Pave   NaN      Reg   \n",
       "2      3          50       RM         50.0     6000   Pave   NaN      Reg   \n",
       "3      4          20       RL         75.0     9750   Pave   NaN      Reg   \n",
       "4      5          20       RL         78.0    10140   Pave   NaN      Reg   \n",
       "..   ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "255  256          60       RL         62.0     7917   Pave  Pave      Reg   \n",
       "256  257          20       RL         85.0    13175   Pave  Pave      Reg   \n",
       "257  258          70       RL         66.0     9042   Pave  Pave      Reg   \n",
       "258  259          20       RL         68.0     9717   Pave  Pave      Reg   \n",
       "259  260          20       RL         75.0     9937   Pave  Pave      Reg   \n",
       "\n",
       "    LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0           Lvl    AllPub  ...           0        0    NaN    NaN        Shed   \n",
       "1           Lvl    AllPub  ...           0        0    NaN    NaN        Shed   \n",
       "2           Lvl    AllPub  ...           0        0    NaN    NaN        Shed   \n",
       "3           Lvl    AllPub  ...           0        0    NaN    NaN        Shed   \n",
       "4           Lvl    AllPub  ...           0        0    NaN  MnPrv        Shed   \n",
       "..          ...       ...  ...         ...      ...    ...    ...         ...   \n",
       "255         Lvl    AllPub  ...           0        0     Gd   GdWo        TenC   \n",
       "256         Lvl    AllPub  ...           0        0     Gd  MnPrv        TenC   \n",
       "257         Lvl    AllPub  ...           0        0     Gd  GdPrv        Shed   \n",
       "258         Lvl    AllPub  ...           0        0     Gd  GdPrv        Shed   \n",
       "259         Lvl    AllPub  ...           0        0     Gd  GdPrv        Shed   \n",
       "\n",
       "    MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0         0      7    2006       Oth        Abnorml  \n",
       "1         0      3    2009        WD         Normal  \n",
       "2         0      5    2009        WD         Normal  \n",
       "3         0     10    2009        WD         Normal  \n",
       "4         0      7    2006        WD         Normal  \n",
       "..      ...    ...     ...       ...            ...  \n",
       "255       0      8    2007        WD         Normal  \n",
       "256       0      2    2010        WD         Normal  \n",
       "257    2500      5    2010        WD         Normal  \n",
       "258       0      4    2010        WD         Normal  \n",
       "259       0      6    2008        WD         Normal  \n",
       "\n",
       "[260 rows x 80 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('test.csv')\n",
    "data = data.fillna(method = 'ffill' , inplace = False )\n",
    "data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20858206-4dfd-4915-8575-2bcf052ac856",
   "metadata": {},
   "source": [
    "remove the nan value / missing value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "50d766e0-f451-44fd-8be0-0157189358bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan(Data,Columns_Name,ImputeValue):\n",
    "    Data[Columns_Name + \"_Imputed\"] =  Data[Columns_Name].fillna(ImputeValue)   \n",
    "median = data.LotFrontage.median\n",
    "impute_nan(data,'LotFrontage',median)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a327e312-4280-4fbb-a28f-5e35062bf98f",
   "metadata": {},
   "source": [
    "catogorical the data = converting str value to int or meachine readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "bbd5108b-c027-45f9-a0ce-7b58202870e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Normalization(\n",
    "    axis=-1, mean=None, variance=None, invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "eb7a697d-78ac-442b-a2c4-369bda162a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.values\n",
    "X = dataset[:,:-1]\n",
    "y = dataset[:,2]\n",
    "X = X.astype(str)\n",
    "y = y.reshape((len(y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5c29a0e9-fe27-4310-a132-7d0ee34ad1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(y_train,y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_e = le.transform(y_train)\n",
    "    y_test_e = le.transform(y_test)\n",
    "    return y_train_e , y_test_e\n",
    "\n",
    "\n",
    "y_train_e, y_test_e = target(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "873e8f09-2bde-445a-90bc-b6f8e89bb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_input(y_train, y_test):\n",
    "\tohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\tohe.fit(y_train)\n",
    "\tX_train_enc = ohe.transform(y_train)\n",
    "\tX_test_enc = ohe.transform(y_test)\n",
    "\treturn X_train_enc, X_test_enc\n",
    "X_train_e, X_test_e = pre_input(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "22856bce-8c55-4de9-84c5-c1c35ebbbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30 , random_state = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dade96f8-8117-493f-b761-adc459cf3a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train size is :  182 182\n",
      "the test size is :  78 78\n",
      "the train and set between 0.00392156862745098 0.00392156862745098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(182, 2114)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the train size is : ' , X_train.shape[0] , y_train.shape[0])\n",
    "print('the test size is : ' , X_test.shape[0] , y_test.shape[0])\n",
    "print('the train and set between' , np.max(X_train) , np.max(X_test))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b66f6d55-a1ea-4c6b-a050-9f595537250c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data\n",
    "X_train = X_train_e / 255.0\n",
    "X_test = X_test_e / 255.0\n",
    "\n",
    "# One hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train_e, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_e, num_classes=10)\n",
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a6814158-3db6-4185-8902-03155dcb360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(10, input_dim=X_train_e.shape[1], activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2f20bb2b-49ff-4f8d-a53e-0fc2147a2f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sobha\\anaconda3\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_3/dense_8/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_3/dense_8/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_3/dense_8/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - loss: -5.0056e-01 - accuracy: 0.0275 - 1s/epoch - 124ms/step\n",
      "Epoch 2/100\n",
      "12/12 - 0s - loss: -3.6456e+00 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "12/12 - 0s - loss: -6.6410e+00 - accuracy: 0.0385 - 41ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "12/12 - 0s - loss: -9.8406e+00 - accuracy: 0.0385 - 39ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "12/12 - 0s - loss: -1.3246e+01 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "12/12 - 0s - loss: -1.6949e+01 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "12/12 - 0s - loss: -2.1005e+01 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "12/12 - 0s - loss: -2.5472e+01 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "12/12 - 0s - loss: -3.0184e+01 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "12/12 - 0s - loss: -3.5355e+01 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "12/12 - 0s - loss: -4.0904e+01 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "12/12 - 0s - loss: -4.6863e+01 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "12/12 - 0s - loss: -5.3121e+01 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "12/12 - 0s - loss: -5.9799e+01 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "12/12 - 0s - loss: -6.6888e+01 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "12/12 - 0s - loss: -7.4330e+01 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 17/100\n",
      "12/12 - 0s - loss: -8.2147e+01 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "12/12 - 0s - loss: -9.0358e+01 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 19/100\n",
      "12/12 - 0s - loss: -9.8919e+01 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 20/100\n",
      "12/12 - 0s - loss: -1.0798e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 21/100\n",
      "12/12 - 0s - loss: -1.1737e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 22/100\n",
      "12/12 - 0s - loss: -1.2719e+02 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 23/100\n",
      "12/12 - 0s - loss: -1.3739e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 24/100\n",
      "12/12 - 0s - loss: -1.4800e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 25/100\n",
      "12/12 - 0s - loss: -1.5913e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 26/100\n",
      "12/12 - 0s - loss: -1.7051e+02 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 27/100\n",
      "12/12 - 0s - loss: -1.8221e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 28/100\n",
      "12/12 - 0s - loss: -1.9451e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 29/100\n",
      "12/12 - 0s - loss: -2.0703e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 30/100\n",
      "12/12 - 0s - loss: -2.2015e+02 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 31/100\n",
      "12/12 - 0s - loss: -2.3353e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 32/100\n",
      "12/12 - 0s - loss: -2.4731e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 33/100\n",
      "12/12 - 0s - loss: -2.6154e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 34/100\n",
      "12/12 - 0s - loss: -2.7622e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 35/100\n",
      "12/12 - 0s - loss: -2.9118e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 36/100\n",
      "12/12 - 0s - loss: -3.0644e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 37/100\n",
      "12/12 - 0s - loss: -3.2211e+02 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 38/100\n",
      "12/12 - 0s - loss: -3.3829e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 39/100\n",
      "12/12 - 0s - loss: -3.5497e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 40/100\n",
      "12/12 - 0s - loss: -3.7184e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 41/100\n",
      "12/12 - 0s - loss: -3.8927e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 42/100\n",
      "12/12 - 0s - loss: -4.0678e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 43/100\n",
      "12/12 - 0s - loss: -4.2484e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 44/100\n",
      "12/12 - 0s - loss: -4.4323e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 45/100\n",
      "12/12 - 0s - loss: -4.6205e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "12/12 - 0s - loss: -4.8144e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 47/100\n",
      "12/12 - 0s - loss: -5.0084e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 48/100\n",
      "12/12 - 0s - loss: -5.2091e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 49/100\n",
      "12/12 - 0s - loss: -5.4127e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "12/12 - 0s - loss: -5.6200e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 51/100\n",
      "12/12 - 0s - loss: -5.8300e+02 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 52/100\n",
      "12/12 - 0s - loss: -6.0459e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 53/100\n",
      "12/12 - 0s - loss: -6.2614e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 54/100\n",
      "12/12 - 0s - loss: -6.4860e+02 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 55/100\n",
      "12/12 - 0s - loss: -6.7106e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 56/100\n",
      "12/12 - 0s - loss: -6.9387e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 57/100\n",
      "12/12 - 0s - loss: -7.1721e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 58/100\n",
      "12/12 - 0s - loss: -7.4067e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 59/100\n",
      "12/12 - 0s - loss: -7.6507e+02 - accuracy: 0.0385 - 32ms/epoch - 3ms/step\n",
      "Epoch 60/100\n",
      "12/12 - 0s - loss: -7.8933e+02 - accuracy: 0.0385 - 32ms/epoch - 3ms/step\n",
      "Epoch 61/100\n",
      "12/12 - 0s - loss: -8.1397e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 62/100\n",
      "12/12 - 0s - loss: -8.3945e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 63/100\n",
      "12/12 - 0s - loss: -8.6473e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 64/100\n",
      "12/12 - 0s - loss: -8.9050e+02 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 65/100\n",
      "12/12 - 0s - loss: -9.1696e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 66/100\n",
      "12/12 - 0s - loss: -9.4316e+02 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 67/100\n",
      "12/12 - 0s - loss: -9.7002e+02 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 68/100\n",
      "12/12 - 0s - loss: -9.9714e+02 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 69/100\n",
      "12/12 - 0s - loss: -1.0246e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 70/100\n",
      "12/12 - 0s - loss: -1.0525e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 71/100\n",
      "12/12 - 0s - loss: -1.0803e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 72/100\n",
      "12/12 - 0s - loss: -1.1089e+03 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 73/100\n",
      "12/12 - 0s - loss: -1.1376e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 74/100\n",
      "12/12 - 0s - loss: -1.1669e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 75/100\n",
      "12/12 - 0s - loss: -1.1965e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 76/100\n",
      "12/12 - 0s - loss: -1.2262e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 77/100\n",
      "12/12 - 0s - loss: -1.2565e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 78/100\n",
      "12/12 - 0s - loss: -1.2873e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 79/100\n",
      "12/12 - 0s - loss: -1.3182e+03 - accuracy: 0.0385 - 38ms/epoch - 3ms/step\n",
      "Epoch 80/100\n",
      "12/12 - 0s - loss: -1.3497e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 81/100\n",
      "12/12 - 0s - loss: -1.3813e+03 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 82/100\n",
      "12/12 - 0s - loss: -1.4136e+03 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 83/100\n",
      "12/12 - 0s - loss: -1.4456e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 84/100\n",
      "12/12 - 0s - loss: -1.4780e+03 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 85/100\n",
      "12/12 - 0s - loss: -1.5109e+03 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 86/100\n",
      "12/12 - 0s - loss: -1.5438e+03 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 87/100\n",
      "12/12 - 0s - loss: -1.5774e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 88/100\n",
      "12/12 - 0s - loss: -1.6114e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 89/100\n",
      "12/12 - 0s - loss: -1.6457e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 90/100\n",
      "12/12 - 0s - loss: -1.6801e+03 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 91/100\n",
      "12/12 - 0s - loss: -1.7147e+03 - accuracy: 0.0385 - 36ms/epoch - 3ms/step\n",
      "Epoch 92/100\n",
      "12/12 - 0s - loss: -1.7502e+03 - accuracy: 0.0385 - 33ms/epoch - 3ms/step\n",
      "Epoch 93/100\n",
      "12/12 - 0s - loss: -1.7856e+03 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 94/100\n",
      "12/12 - 0s - loss: -1.8213e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 95/100\n",
      "12/12 - 0s - loss: -1.8577e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 96/100\n",
      "12/12 - 0s - loss: -1.8941e+03 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "Epoch 97/100\n",
      "12/12 - 0s - loss: -1.9311e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 98/100\n",
      "12/12 - 0s - loss: -1.9678e+03 - accuracy: 0.0385 - 35ms/epoch - 3ms/step\n",
      "Epoch 99/100\n",
      "12/12 - 0s - loss: -2.0054e+03 - accuracy: 0.0385 - 34ms/epoch - 3ms/step\n",
      "Epoch 100/100\n",
      "12/12 - 0s - loss: -2.0428e+03 - accuracy: 0.0385 - 37ms/epoch - 3ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000182102B79D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "INFO:tensorflow:Assets written to: test(estimate).csv\\assets\n",
      "prices [-2010.3211669921875, 0.03846153989434242]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_e, y_train_e, epochs=100, batch_size=16, verbose=2)\n",
    "price = model.evaluate(X_test_e, y_test_e, verbose=0)\n",
    "model.save('test(estimate).csv')\n",
    "print('prices',price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ca5e4ec9-a393-4a89-8740-0280f626a155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18217d80fa0>]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(price)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
